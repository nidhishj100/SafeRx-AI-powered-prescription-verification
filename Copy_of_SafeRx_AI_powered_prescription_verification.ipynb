{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üèÜ SafeRx : AI powered prescription verification  \n",
        "### Smart AI-based Prescription Verification using IBM Granite + OCR  \n",
        "üë®‚Äç‚öïÔ∏è Problem: Manual prescription checking is slow & error-prone ‚Üí risks patient safety.  \n",
        "ü§ñ Solution: AI pipeline that reads handwritten prescriptions, identifies medicines, checks safety & dosage, and raises alerts.  \n",
        "üí° Tech: Tesseract OCR + IBM Granite LLM + Hugging Face + Alert System.  "
      ],
      "metadata": {
        "id": "jD-o1SqfCo0M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6rAk0azCCij"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate torch pillow pytesseract\n",
        "!sudo apt install -y tesseract-ocr\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# üîë Use your Hugging Face token here\n",
        "login(\"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\")\n"
      ],
      "metadata": {
        "id": "18J_XFVdDQ92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload handwritten doctor prescription\n",
        "uploaded = files.upload()\n",
        "image_path = list(uploaded.keys())[0]\n",
        "print(\"üìÇ Uploaded file:\", image_path)\n"
      ],
      "metadata": {
        "id": "vARwhiLbDUOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "\n",
        "def extract_prescription_text(image_path):\n",
        "    # Load image with OpenCV\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply thresholding to make text stand out\n",
        "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
        "\n",
        "    # Remove noise (morphological operations)\n",
        "    kernel = np.ones((1, 1), np.uint8)\n",
        "    clean = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "    # OCR with better configs for handwriting\n",
        "    config = \"--psm 6 --oem 3\"\n",
        "    text = pytesseract.image_to_string(clean, config=config)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "prescription_text = extract_prescription_text(image_path)\n",
        "print(\"üìù Extracted Prescription Text:\\n\", prescription_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "08fPktWxDa1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_ID = \"ibm-granite/granite-3.3-2b-instruct\"\n",
        "\n",
        "print(\"üîÑ Loading IBM Granite model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_auth_token=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "def granite_chat(user_message, max_new_tokens=300):\n",
        "    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n"
      ],
      "metadata": {
        "id": "HjOMT3jeDeBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_prescription(prescription_text):\n",
        "    prompt = f\"\"\"\n",
        "    You are a trusted medical AI assistant.\n",
        "\n",
        "    Here is a doctor's handwritten prescription:\n",
        "\n",
        "    {prescription_text}\n",
        "\n",
        "    For each medicine:\n",
        "    - Identify the drug name\n",
        "    - State its generic name\n",
        "    - Mention common uses\n",
        "    - Usual adult dosage\n",
        "    - Possible side effects\n",
        "    - Safety notes / warnings\n",
        "\n",
        "    Format the output as a clean, structured table.\n",
        "    \"\"\"\n",
        "    return granite_chat(prompt, max_new_tokens=500)\n",
        "\n",
        "report = explain_prescription(prescription_text)\n",
        "print(\"üìã Prescription Analysis Report:\\n\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "UdC4ZuxLDh9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "def display_report(report):\n",
        "    display(Markdown(\"### üìã AI Prescription Guardian Report\\n\" + report))\n",
        "\n",
        "display_report(report)\n"
      ],
      "metadata": {
        "id": "8BT73MGpDk69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def safety_check_fast(prescription_text):\n",
        "    prompt = f\"\"\"\n",
        "    Prescription safety quick check:\n",
        "\n",
        "    {prescription_text}\n",
        "\n",
        "    Answer in one line only:\n",
        "    - If safe ‚Üí reply: \"‚úÖ Safe\"\n",
        "    - If unsafe ‚Üí reply: \"‚ùå Risk: [short reason]\"\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üîé Running quick safety check...\")\n",
        "    return granite_chat(prompt, max_new_tokens=120)  # lower tokens for speed\n",
        "\n",
        "# Example usage\n",
        "safety_report = safety_check_fast(prescription_text)\n",
        "print(safety_report)\n"
      ],
      "metadata": {
        "id": "HUW6Btw7DteT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}